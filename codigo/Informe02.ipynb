{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VABBBPhEJn0M"
   },
   "source": [
    "# Instrucciones del Informe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta tarea, implementará técnicas de preprocesamiento de texto, métodos de codificación numérica de texto y funciones de clasificación.\n",
    "\n",
    "Usaremos dos conjuntos de datos de texto:\n",
    "- El primer conjunto de datos es una submuestra de un [Conjunto de datos de clickbait](https://github.com/bhargaviparanjape/clickbait/tree/master/dataset) que tiene títulos de artículos y una etiqueta binaria que indica si el título se considera clickbait.\n",
    "- El segundo conjunto de datos es una submuestra de un [Conjunto de datos de Web of Science](https://data.mendeley.com/datasets/9rw3vkcfy4/6) que tiene artículos y una etiqueta correspondiente en el dominio de los artículos.\n",
    "\n",
    "Las técnicas de preprocesamiento de texto limpiarán estos dos conjuntos de datos y el resultado se introducirá en los métodos de codificación numérica. Una vez que tengamos el texto en un formato codificado numéricamente, introduciremos los valores codificados en las funciones de clasificación y predeciremos la etiqueta de los artículos/títulos.\n",
    "\n",
    "<!-- <img src=\"/data/images/sequence.png\" width=\"75%\"> -->\n",
    "<p align=\"center\"><img src=\"data/images/sequence.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "Para finalizar la tarea, también implementaremos y exploraremos varias métricas de evaluación para analizar el rendimiento de los clasificadores.\n",
    "\n",
    "Como sección de crédito adicional, hemos incluido una sección que profundiza en la transmisión y el preprocesamiento de un gran conjunto de datos utilizando la popular biblioteca [Hugging Face](https://huggingface.co/). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entregables y distribución de puntos\n",
    "\n",
    "### Q0: División de validación [5 puntos]\n",
    "- **0.1: División de datos de prueba y entrenamiento** [5 puntos] Entregables: <font color = 'green'>split.py</font>\n",
    "\n",
    "- [5 puntos] split_data\n",
    "\n",
    "### Q1: Preprocesamiento de texto [15 puntos]\n",
    "- **1.1: Eliminación de ruido, tokenización y normalización** [10 puntos] Entregables: <font color = 'green'>preprocess.py</font>\n",
    "\n",
    "- [5 puntos] clean_text\n",
    "\n",
    "- [5 puntos] clean_dataset\n",
    "- **1.3: Limpieza del conjunto de datos de Web of Science** [5 puntos] Entregables: <font color = 'green'>preprocess.py</font>\n",
    "\n",
    "- [5 puntos] clean_wos\n",
    "\n",
    "### Q2: Codificación numérica [30 puntos]\n",
    "- **2.1: Codificación One Hot y Bolsa de palabras** [20 pts] Entregables: <font color = 'green'>bagofwords.py</font>\n",
    "\n",
    "- [1pts] \\__init__\n",
    "\n",
    "- [3pts] split_text\n",
    "\n",
    "- [3pts] flatten_text\n",
    "\n",
    "- [5pts] fit\n",
    "\n",
    "- [3pts] onehot\n",
    "\n",
    "- [5pts] transform\n",
    "- **2.2: Bolsa de palabras con CountVectorizer** [5 pts] Entregables: <font color = 'green'>encode.py</font>\n",
    "\n",
    "- [1pts] \\__init__\n",
    "\n",
    "- [2pts] fit\n",
    "\n",
    "- [2pts] transform\n",
    "- **2.3: Codificación TF-IDF** [5 pts] Entregables: <font color = 'green'>encode.py</font>\n",
    "\n",
    "- [1pts] \\__init__\n",
    "\n",
    "- [2pts] fit\n",
    "\n",
    "- [2pts] transform\n",
    "\n",
    "### Q3: Clasificación mediante Naive Bayes [10 puntos]\n",
    "- **3.1: Implementación de Naive Bayes multinomial** [5 puntos] Entregables: <font color = 'green'>classify.py</font>\n",
    "\n",
    "- [1 punto] \\__init__\n",
    "\n",
    "- [2 puntos] ajuste\n",
    "\n",
    "- [2 puntos] predicción\n",
    "- **3.2: Implementación de Naive Bayes gaussiano** [5 puntos] Entregables: <font color = 'green'>classify.py</font>\n",
    "\n",
    "- [1 punto] \\__init__\n",
    "\n",
    "- [2 puntos] ajuste\n",
    "\n",
    "- [2 puntos] predicción\n",
    "\n",
    "### Q4: Métricas de evaluación [15 puntos]\n",
    "- **4.1: Implementación de precisión, recuperación, exactitud, puntuación F1, puntuación ROC_AUC, matriz de confusión** [15 puntos] Entregables: <font color = 'green'>metrics.py</font>\n",
    "\n",
    "- [5 puntos] precisión\n",
    "\n",
    "- [2 puntos] recuperación\n",
    "\n",
    "- [2 puntos] precisión\n",
    "\n",
    "- [2 puntos] puntuación f1\n",
    "\n",
    "- [2 puntos] puntuación roc_auc\n",
    "\n",
    "- [2 puntos] matriz de confusión\n",
    "\n",
    "### Q5: Preprocesamiento en Big Data (7,5 puntos de crédito adicional adicional)\n",
    "- **5.1 Tokenización de datos transmitidos** Entregables: <font color = 'green'>preprocess_bigdata.py</font> **(Enviar a la sección de crédito adicional adicional en Gradescope)**\n",
    "\n",
    "- [2,5 puntos] \\__init__\n",
    "\n",
    "- [2,5 puntos] tokenizar\n",
    "\n",
    "- [2,5 puntos] preprocess_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuración\n",
    "**Consulte el archivo environment_setup.md para crear el entorno para esta tarea.** Este cuaderno se prueba con las versiones de paquetes que se indican en la salida de la celda Importaciones de la biblioteca a continuación, y los paquetes correspondientes se pueden descargar desde [miniconda](https://docs.conda.io/en/latest/miniconda.html). También puede que desee familiarizarse con varios paquetes:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [sklearn](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "**Otra alternativa es la instalación de las librerias mediante PIP.**\n",
    "\n",
    "* pip install matplotlib numpy scikit-learn scipy pandas nltk bs4 mkl<2022 datasets transformers\n",
    "\n",
    "En los archivos .py, implemente las funciones que tienen `raise NotImplementedError` y, después de terminar la codificación, elimine o comente `raise NotImplementedError`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSvrBBuvJn0O"
   },
   "source": [
    "## Importando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8164,
     "status": "ok",
     "timestamp": 1660331673596,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "F-RA4NJLLqW3",
    "outputId": "c0da60f4-8df3-4991-8f06-4caeb078138e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Version Information\n",
      "python: 3.11.14 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 18:30:03) [MSC v.1929 64 bit (AMD64)]\n",
      "pandas: 2.3.3\n",
      "numpy: 1.24.3\n",
      "scipy: 1.10.1\n",
      "re: 2.2.1\n",
      "bs4: 4.13.5\n",
      "nltk: 3.9.2\n",
      "sklearn: 1.7.2\n",
      "matplotlib: 3.10.6\n",
      "datasets: 3.3.2\n",
      "transformers: 4.51.3\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys, re, bs4, nltk, sklearn, matplotlib\n",
    "\n",
    "# Used for Q5 Bonus only\n",
    "import datasets, transformers \n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version Information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('scipy: {}'.format(sp.__version__))\n",
    "print('re: {}'.format(re.__version__))\n",
    "print('bs4: {}'.format(bs4.__version__))\n",
    "print('nltk: {}'.format(nltk.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('datasets: {}'.format(datasets.__version__)) # Q5 Bonus\n",
    "print('transformers: {}'.format(transformers.__version__)) #Q5 Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaJTlnqRJn0P"
   },
   "source": [
    "# Carga de Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7HnCglVMJn0P"
   },
   "source": [
    "Comenzamos cargando ambos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "df_data = pd.read_csv('./data/data.csv')\n",
    "df_data_wos = pd.read_csv('./data/data_wos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra la cantidad de titulares en el conjunto de datos, así como una muestra de los titulares de los artículos y su etiqueta binaria, donde 0 se considera que no es clickbait y 1 es clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Headlines: 24000\n",
      "\n",
      "\n",
      "Sample Label and Headlines:\n",
      "27 Breathtaking Alternatives To A Traditional Wedding Bouquet <br>\n",
      ": 1\n",
      "22 Pictures People Who Aren't Grad Students Will <strong>Never</strong> Understand\n",
      ": 1\n",
      "PepsiCo Profit Falls 43 Percent\n",
      ": 0\n",
      "Website of Bill O'Reilly, FOX News commentator, hacked in retribution\n",
      ": 0\n",
      "The Green Toy Soldiers From Your Childhood Now Come In Baller Yoga Poses A\n",
      ": 1\n",
      "\n",
      "Output of Sample Headlines without Print Statement:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>27 Breathtaking Alternatives To A Traditional ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>22 Pictures People Who Aren't Grad Students Wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>PepsiCo Profit Falls 43 Percent\\r\\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Website of Bill O'Reilly, FOX News commentator...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>The Green Toy Soldiers From Your Childhood Now...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headline  label\n",
       "105  27 Breathtaking Alternatives To A Traditional ...      1\n",
       "106  22 Pictures People Who Aren't Grad Students Wi...      1\n",
       "107                PepsiCo Profit Falls 43 Percent\\r\\n      0\n",
       "108  Website of Bill O'Reilly, FOX News commentator...      0\n",
       "109  The Green Toy Soldiers From Your Childhood Now...      1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Number of Headlines: {len(df_data)}')\n",
    "\n",
    "print('\\n\\nSample Label and Headlines:')\n",
    "x = 105\n",
    "for label, line in zip(df_data['headline'][x:x+5], df_data['label'][x:x+5]):\n",
    "    print(f'{label}: {line}')\n",
    "    \n",
    "print('\\nOutput of Sample Headlines without Print Statement:')\n",
    "df_data[['headline', 'label']][x:x+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Articles: 2000\n",
      "\n",
      "Label Key: {0: 'CS', 1: 'ECE', 4: 'Civil', 5: 'Medical'}\n",
      "\n",
      "Sample Label and Articles:\n",
      "\n",
      "0 - CS: An efficient procedure for calculating the electromagnetic fields in multilayered cylindrical structures is reported in this paper. Using symbolic computation, spectral Green's functions, suitable for numerical implementations are determined in compact and closed forms. Applications are presented for structures with two dielectric layers.\n",
      "\n",
      "1 - ECE: A multifunctional platform based on the microhotplate was developed for applications including a Pirani vacuum gauge, temperature, and gas sensor. It consisted of a tungsten microhotplate and an on-chip operational amplifier. The platform was fabricated in a standard complementary metal oxide semiconductor (CMOS) process. A tungsten plug in standard CMOS process was specially designed as the serpentine resistor for the microhotplate, acting as both heater and thermister. With the sacrificial layer technology, the microhotplate was suspended over the silicon substrate with a 340 nm gap. The on-chip operational amplifier provided a bias current for the microhotplate. This platform has been used to develop different kinds of sensors. The first one was a Pirani vacuum gauge ranging from 10(-1) to 10(5) Pa. The second one was a temperature sensor ranging from -20 to 70 degrees C. The third one was a thermal-conductivity gas sensor, which could distinguish gases with different thermal conductivities in constant gas pressure and environment temperature. In the fourth application, with extra fabrication processes including the deposition of gas-sensitive film, the platform was used as a metal-oxide gas sensor for the detection of gas concentration.\n",
      "\n",
      "4 - Civil: Artificial neural networks have been effectively used in various civil engineering fields, including construction management and labour productivity. In this study, the performance of the feed forward neural network (FFNN) was compared with radial basis neural network (RBNN) in modelling the productivity of masonry crews. A variety of input factors were incorporated and analysed. Mean absolute percentage error (MAPE) and correlation coefficient (R) were used to evaluate model performance. Research results indicated that the neural computing techniques could be successfully employed in modelling crew productivity. It was also found that successful models could be developed with different combinations of input factors, and several of the models which excluded one or more input factors turned out to be better than the baseline models. Based on the MAPE values obtained for the models, the RBNN technique was found to be better than the FFNN technique, although both slightly overestimated the masons' productivity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Articles: {len(df_data_wos)}')\n",
    "\n",
    "# Numerical label to domain mapping\n",
    "wos_label = {0:'CS', 1:'ECE', 4:'Civil', 5:'Medical'}\n",
    "\n",
    "print('\\nLabel Key:', wos_label)\n",
    "\n",
    "print('\\nSample Label and Articles:\\n')\n",
    "x = 107\n",
    "for label, line in zip(df_data_wos['label'][x:x+3], df_data_wos['article'][x:x+3]):\n",
    "    print(f'{label} - {wos_label[label]}: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q0: División de validación [5pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 0.1: Dividir datos de prueba y entrenamiento [5pts]\n",
    "En el archivo **split.py** complete las siguientes funciones:\n",
    "\n",
    "* **split_data**: Divida los datos en una división de prueba y entrenamiento 80/20.\n",
    "### 0.1.1 Pruebas locales para funciones divididas [Sin puntos]\n",
    "Puede probar su implementación de las funciones contenidas en **split.py** en la celda a continuación. No dude en comentar las pruebas para las funciones que aún no se hayan completado. Consulte [Uso de pruebas locales](#using_local_tests) para obtener más detalles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Split Functions \n",
      "\n",
      "Your split_data works for clickbait as expected:  True\n",
      "Your split_data works for wos as expected:  True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "from local_tests.split_test import Split_test\n",
    "\n",
    "test_sd = Split_test()\n",
    "\n",
    "print('Local Tests for Split Functions \\n')\n",
    "# Local test for split_data\n",
    "train_clickbait, test_clickbait = split.split_data(df_data)\n",
    "\n",
    "split_clickbait = (len(train_clickbait) == test_sd.x_train_len) and (len(test_clickbait) == test_sd.x_test_len)\n",
    "print('Your split_data works for clickbait as expected: ', split_clickbait)\n",
    "\n",
    "# Local test for split_data\n",
    "train_wos, test_wos = split.split_data(df_data_wos)\n",
    "\n",
    "split_wos = (len(train_wos) == test_sd.x_train_wos_len) and (len(test_wos) == test_sd.x_test_wos_len)\n",
    "print('Your split_data works for wos as expected: ', split_wos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1.2: División de los conjuntos de datos [Sin puntos]\n",
    "Ejecute las celdas a continuación para dividir el conjunto de datos de prueba y entrenamiento utilizando la función de división que ya implementó en 0.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "_mjycOH0Jn0Q"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "\n",
    "df_train, df_test = split.split_data(df_data)\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train, y_train = list(df_train['headline']), list(df_train['label'])\n",
    "x_test, y_test = list(df_test['headline']), list(df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VajBVO_OJn0Q"
   },
   "source": [
    "Below is the number of headlines in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660193237405,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "xbSkLLTxJn0R",
    "outputId": "55a86509-f31d-4c29-f2c5-795aef2cf588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Headlines: 19200\n",
      "Number of Test Headlines: 4800\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Train Headlines: {len(x_train)}')\n",
    "print(f'Number of Test Headlines: {len(x_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "SWNNCLE9Jn0R"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import split\n",
    "\n",
    "df_train_wos, df_test_wos = split.split_data(df_data_wos)\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train_wos, y_train_wos = list(df_train_wos['article']), list(df_train_wos['label'])\n",
    "x_test_wos, y_test_wos = list(df_test_wos['article']), list(df_test_wos['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660193238898,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "WSg8NO1AJn0R",
    "outputId": "0b419f71-54de-4990-c4f7-a626f2b358bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Articles: 1600\n",
      "Number of Test Articles: 400\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of Train Articles: {len(x_train_wos)}')\n",
    "print(f'Number of Test Articles: {len(x_test_wos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: Preprocesamiento de texto [15 puntos]\n",
    "## 1.1: Eliminación de ruido, tokenización y normalización [10 puntos]\n",
    "En el archivo **preprocess.py**, complete las siguientes funciones:\n",
    "* <strong>clean_text</strong>: limpie una sola cadena de entrada dada, los requisitos están en la cadena de documentación\n",
    "* <strong>clean_dataset</strong>: use clean_text para limpiar un conjunto de datos completo de cadenas\n",
    "\n",
    "Sugerencia:\n",
    "* Puede encontrar <a href=\"https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\">esta discusión</a> útil para eliminar la etiqueta html.\n",
    "### 1.1.1 Pruebas locales para funciones de preprocesamiento [Sin puntos]\n",
    "Puede probar su implementación de las funciones contenidas en **preprocess.py** en la celda a continuación. No dude en comentar las pruebas de las funciones que aún no se han completado. Consulte [Uso de pruebas locales](#using_local_tests) para obtener más detalles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nicol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 1025,
     "status": "ok",
     "timestamp": 1660332368105,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kWrhNORTJn0S",
    "outputId": "9a06e556-ce2c-4686-f7fa-2d3220f419bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Preprocess Functions \n",
      "\n",
      "Your clean_text works as expected: False\n",
      "Your clean_dataset works as expected: False\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import Preprocess\n",
    "from local_tests.preprocess_test import Preprocess_Test\n",
    "\n",
    "test_pp = Preprocess_Test()\n",
    "pp = Preprocess()\n",
    "\n",
    "print('Local Tests for Preprocess Functions \\n')\n",
    "# Local test for clean_text\n",
    "output = []\n",
    "for text in test_pp.x_train:\n",
    "    output.append(pp.clean_text(text))\n",
    "clean_text_test = (output == test_pp.cleaned_text)\n",
    "print('Your clean_text works as expected:', clean_text_test)\n",
    "\n",
    "# Local test for clean_dataset\n",
    "output = pp.clean_dataset(test_pp.x_train)\n",
    "clean_dataset_test = (output == test_pp.cleaned_text)\n",
    "print('Your clean_dataset works as expected:', clean_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Limpieza del conjunto de datos de Clickbait [Sin puntos]\n",
    "Ejecute la celda a continuación para limpiar el conjunto de datos de prueba y entrenamiento de Clickbait utilizando las funciones de preprocesamiento que ya ha implementado en 1.1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Rbg8ChJdJn0T"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import Preprocess\n",
    "cleaned_text = Preprocess().clean_dataset(x_train)\n",
    "cleaned_text_test = Preprocess().clean_dataset(x_test)\n",
    "\n",
    "# Do not worry if you see a warning from BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Limpieza del conjunto de datos de Web of Science [5 puntos]\n",
    "En el archivo **preprocess.py**, complete la siguiente función utilizando las funciones de preprocesamiento que ya ha implementado en 1.1:\n",
    "* <strong>clean_wos</strong>: Limpia una única cadena de entrada dada, los requisitos están en la cadena de documentación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zjPc0qMEJn0T"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess import clean_wos\n",
    "cleaned_text_wos, cleaned_text_wos_test = clean_wos(x_train_wos, x_test_wos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2: Codificación numérica [30 puntos]\n",
    "En esta sección, exploraremos la codificación One Hot, Bag of Words y TF-IDF como métodos de codificación de texto a numérico. ## 2.1: Codificación One Hot y Bag of Words [20pts]\n",
    "En el archivo **bagofwords.py**, complete las siguientes funciones:\n",
    "* <strong>\\__init__</strong>\n",
    "* <strong>split_text</strong>\n",
    "* <strong>flatten_text</strong>\n",
    "* <strong>fit</strong>\n",
    "* <strong>onehot</strong>\n",
    "* <strong>transform</strong>\n",
    "\n",
    "Dado el texto Clickbait limpio de la pregunta 1, convierta cada cadena en un vector one hot y luego use la representación del vector one hot de la cadena para crear una representación de bag of words.\n",
    "\n",
    "Por ejemplo, dados los siguientes textos limpios:\n",
    "\n",
    "`[\"perro salta valla\", \"perro rompe valla\"]`\n",
    "\n",
    "Para crear una representación one hot, las cadenas deben descomponerse en una lista de palabras:\n",
    "\n",
    "`[['perro', 'saltar', 'valla'], ['perro', 'romper', 'valla']]`\n",
    "\n",
    "Una vez que la cadena se ha descompuesto en una lista de palabras, la lista deberá aplanarse para crear una asignación entre una palabra única y una representación one hot única:\n",
    "\n",
    "`['perro', 'saltar', 'valla', 'perro', 'romper', 'valla']`\n",
    "\n",
    "Al introducir la lista aplanada para que se ajuste al codificador One Hot, se creará la siguiente asignación:\n",
    "\n",
    "`[1., 0., 0., 0.]` $\\rightarrow$ break  \n",
    "`[0., 1., 0., 0.]` $\\rightarrow$ dog  \n",
    "`[0., 0., 1., 0.]` $\\rightarrow$ fence  \n",
    "`[0., 0., 0., 1.]` $\\rightarrow$ jump  \n",
    "\n",
    "Ahora que tenemos la asignación, podemos obtener la representación one hot de `\"dog jump fence\"`:\n",
    "\n",
    "`[[0., 1., 0., 0.],\n",
    "[0., 0., 0., 1.],\n",
    "[0., 0., 1., 0.]]`\n",
    "\n",
    "La representación one hot se puede sumar a lo largo de las filas para crear la representación bag of words para `\"dog jump fence\"`:\n",
    "\n",
    "`[0., 1., 1., 1.]`\n",
    "### 2.1.1 Pruebas locales para codificación One Hot y Bag of Words [Sin puntos]\n",
    "Puede probar su implementación de las funciones contenidas en **bagofwords.py** en la celda a continuación. No dude en comentar las pruebas de las funciones que aún no se han completado. Consulte [Uso de pruebas locales](#using_local_tests) para obtener más detalles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 1910,
     "status": "ok",
     "timestamp": 1660193319783,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "UOS9EcVrJn0U",
    "outputId": "b80968c7-67a8-4431-da4f-98be96fea09e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for OHE_BOW Functions \n",
      "\n",
      "Your split_text works as expected: True\n",
      "Your flatten_list works as expected: True\n",
      "Your fit works as expected: True\n",
      "Your onehot works as expected: True\n",
      "Your transform works as expected: True\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from bagofwords import OHE_BOW\n",
    "from local_tests.bagofwords_test import BagofWords_Test\n",
    "\n",
    "test_ob = BagofWords_Test()\n",
    "ob = OHE_BOW()\n",
    "\n",
    "print('Local Tests for OHE_BOW Functions \\n')\n",
    "# Local test for split_text\n",
    "output = ob.split_text(test_ob.cleaned_text)\n",
    "split_text_test = (output == test_ob.data_split)\n",
    "print('Your split_text works as expected:', split_text_test)\n",
    "\n",
    "# Local test for flatten_list\n",
    "output = ob.flatten_list(test_ob.data_split)\n",
    "flatten_text_test = np.all((output == test_ob.flattened_list) == True)\n",
    "print('Your flatten_list works as expected:', flatten_text_test)\n",
    "\n",
    "# Local test for fit\n",
    "ob.fit(test_ob.cleaned_text)\n",
    "ob_cat_test = np.all((ob.oh.categories_[0] == test_ob.fitted_categories) == True)\n",
    "vocab_size_test = (ob.vocab_size == test_ob.vocab_size)\n",
    "print('Your fit works as expected:', (ob_cat_test and vocab_size_test))\n",
    "\n",
    "# Local test for onehot\n",
    "output = ob.onehot(test_ob.data_split[0])\n",
    "onehot_test = np.all((output == test_ob.encode_0) == True)\n",
    "# vocab_size_test = (ob.vocab_size == test_ob.vocab_size)\n",
    "print('Your onehot works as expected:', onehot_test)\n",
    "\n",
    "# Local test for transform\n",
    "output = ob.transform(test_ob.cleaned_text)\n",
    "onehot_test = np.all((output == test_ob.cleaned_text_bow) == True)\n",
    "print('Your transform works as expected:', onehot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Convertir el texto limpio de Web of Science en una representación de bolsa de palabras [Sin puntos]\n",
    "Ejecute la celda siguiente para convertir el conjunto de datos de prueba y entrenamiento limpios utilizando las funciones de codificación One Hot y bolsa de palabras que ya ha implementado en 2.1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGRdZ2QFJn0U"
   },
   "source": [
    "### 2.1.2 Convert the Web of Science Cleaned Text to a Bag of Words Representation [No Points]\n",
    "Run the below cell to convert the cleaned train and test dataset using the one hot encoding and bag of words functions that you have already implemented in 2.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "VBON22VfJn0V"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# this cell may take a few minutes to run\n",
    "\n",
    "from bagofwords import OHE_BOW\n",
    "\n",
    "ohe_bow = OHE_BOW()\n",
    "ohe_bow.fit(cleaned_text_wos)\n",
    "x_train_ohe_bow = ohe_bow.transform(cleaned_text_wos)\n",
    "x_test_ohe_bow = ohe_bow.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1660193393490,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "4GA_VE9jJn0V",
    "outputId": "0dc3ccef-9b06-4b8c-8da2-abf42a9a4e19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words representation for Web of Science Train: (1600, 17796)\n",
      "Shape of Bag of Words representation for Web of Science Test: (400, 17796)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Bag of Words representation for Web of Science Train:', x_train_ohe_bow.shape)\n",
    "print('Shape of Bag of Words representation for Web of Science Test:', x_test_ohe_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Convertir el texto limpio de Clickbait en una representación de bolsa de palabras [Sin puntos]\n",
    "Ejecute la celda a continuación para convertir el conjunto de datos de prueba y entrenamiento limpios utilizando las funciones de codificación one hot y bolsa de palabras que ya implementó en 2.1.1\n",
    "\n",
    "**NOTA:** La ejecución del código demorará unos minutos y eso es normal. Para que los cálculos sean más rápidos, no dude en usar [ejecutores de pool paralelo](https://superfastpython.com/processpoolexecutor-in-python/) en la función de transformación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bagofwords import OHE_BOW\n",
    "\n",
    "ohe_bow = OHE_BOW()\n",
    "ohe_bow.fit(cleaned_text)\n",
    "x_train_ohe_bow = ohe_bow.transform(cleaned_text)\n",
    "x_test_ohe_bow = ohe_bow.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words representation for clickbait Train: (19200, 17015)\n",
      "Shape of Bag of Words representation for clickbait Test: (4800, 17015)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Bag of Words representation for clickbait Train:', x_train_ohe_bow.shape)\n",
    "print('Shape of Bag of Words representation for clickbait Test:', x_test_ohe_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Bolsa de palabras utilizando CountVectorizer [5 puntos]\n",
    "En el archivo **encode.py**, complete las siguientes funciones en la clase `BagOfWords`:\n",
    "* <strong>\\__init__</strong>\n",
    "* <strong>fit</strong>\n",
    "* <strong>transform</strong>\n",
    "\n",
    "En la sección anterior, implementamos la Bolsa de palabras utilizando la función OneHotEncoder de sklearn. En esta sección, utilizará la función CountVectorizer de sklearn para crear una representación de bolsa de palabras. Descubrirá que la implementación con CountVectorizer es mucho más corta y rápida que con OneHotEncoder de sklearn. No se proporciona ninguna prueba local, sin embargo, sus resultados con CountVectorizer o OneHotEncoder deberían ser similares. La diferencia es que CountVectorizer ignora caracteres individuales y nuestra implementación de One Hot Encoder no lo hace, por lo tanto, la dimensión del vocabulario es ligeramente más pequeña.\n",
    "### 2.2.1 Convertir el texto limpio de Web of Science en una representación de bolsa de palabras [Sin puntos]\n",
    "Ejecute la celda a continuación para convertir el conjunto de datos limpio utilizando las funciones de bolsa de palabras que ya ha implementado en 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1093,
     "status": "ok",
     "timestamp": 1660193394550,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "l7WuxwAYJn0W",
    "outputId": "f6a3395b-4b2e-42c4-def3-75c70c5e96a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 844 ms\n",
      "Wall time: 852 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import BagOfWords\n",
    "\n",
    "bow = BagOfWords()\n",
    "bow.fit(cleaned_text_wos)\n",
    "x_train_bow_wos = bow.transform(cleaned_text_wos)\n",
    "x_test_bow_wos = bow.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1660193394551,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "pRl1a0qgJn0W",
    "outputId": "676465a8-f880-43b4-cab9-bc73edbd2475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words representation for Web of Science Train: (1600, 17777)\n",
      "Shape of Bag of Words representation for Web of Science Test: (400, 17777)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Bag of Words representation for Web of Science Train:', x_train_bow_wos.shape)\n",
    "print('Shape of Bag of Words representation for Web of Science Test:', x_test_bow_wos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Convertir el texto limpio de Clickbait en una representación de bolsa de palabras [Sin puntos]\n",
    "Ejecute la celda a continuación para convertir el conjunto de datos limpio utilizando las funciones de bolsa de palabras que ya ha implementado en 2.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1922,
     "status": "ok",
     "timestamp": 1660193396467,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "FKcBcxppJn0X",
    "outputId": "6cc353f2-ade8-4385-f83a-cb587f575570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 906 ms\n",
      "Wall time: 913 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import BagOfWords\n",
    "\n",
    "bow = BagOfWords()\n",
    "bow.fit(cleaned_text)\n",
    "x_train_bow = bow.transform(cleaned_text)\n",
    "x_test_bow = bow.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1660193396468,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "Lxm_fFrQJn0X",
    "outputId": "de7c0405-7911-40f3-8d67-d022704b5733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Bag of Words representation for Clickbait Train: (19200, 16997)\n",
      "Shape of Bag of Words representation for Clickbait Test: (4800, 16997)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Bag of Words representation for Clickbait Train:', x_train_bow.shape)\n",
    "print('Shape of Bag of Words representation for Clickbait Test:', x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe cómo no usamos la implementación de bolsa de palabras utilizando OneHotEncoder para transformar el conjunto de datos de Clickbait. El uso de la implementación de la versión 2.1 podría llevar una cantidad significativa de tiempo, según la cantidad de recursos computacionales que tengamos. No dude en intentar convertir este conjunto de datos utilizando la implementación 2.1 y ver cuánto tiempo lleva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Codificación TF-IDF [5 puntos]\n",
    "Observe cómo en Bag of Words todas las palabras tienen la misma importancia; sin embargo, en TF-IDF podemos asignar una importancia más lógica a un vector de palabras para cada documento. Puede volver a consultar las diapositivas de la semana 2 para revisar los detalles sobre el concepto de TF-IDF; sin embargo, en esta sección aprovecharemos el TfidfVectorizer de sklearn para codificar nuestros conjuntos de datos.\n",
    "\n",
    "En el archivo **encode.py**, complete las siguientes funciones en la clase `TfIdf`:\n",
    "* <strong>\\__init__</strong>\n",
    "* <strong>fit</strong>\n",
    "* <strong>transform</strong>\n",
    "### 2.3.1 Convertir el texto limpio de Web of Science en una representación TfIdf [Sin puntos]\n",
    "Ejecute la siguiente celda para convertir el conjunto de datos limpio utilizando las funciones TfIdf que ya ha implementado en 2.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "A8d9fjNTJn0X"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import TfIdf\n",
    "\n",
    "tfidf = TfIdf()\n",
    "tfidf.fit(cleaned_text_wos)\n",
    "x_train_tfidf_wos = tfidf.transform(cleaned_text_wos)\n",
    "x_test_tfidf_wos = tfidf.transform(cleaned_text_wos_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 530,
     "status": "ok",
     "timestamp": 1660193397330,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "lnSnmI4RJn0Y",
    "outputId": "7f455270-0cdf-4e6e-82f2-6573d52d9da5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TfIdf representation for Web of Science Train: (1600, 17777)\n",
      "Shape of TfIdf representation for Web of Science Test: (400, 17777)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of TfIdf representation for Web of Science Train:', x_train_tfidf_wos.shape)\n",
    "print('Shape of TfIdf representation for Web of Science Test:', x_test_tfidf_wos.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6ujP6kQJn0Y"
   },
   "source": [
    "### 2.3.2 Convert the Clickbait Cleaned Text to a TfIdf Representation [No Points]\n",
    "Run the below cell to convert the cleaned dataset using the TfIdf functions that you have already implemented in 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "XQ0VGiV6Jn0Y"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from encode import TfIdf\n",
    "\n",
    "tfidf = TfIdf()\n",
    "tfidf.fit(cleaned_text)\n",
    "x_train_tfidf = tfidf.transform(cleaned_text)\n",
    "x_test_tfidf = tfidf.transform(cleaned_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1660193399076,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "ahbi1WNNJn0Y",
    "outputId": "0c570f3b-18ca-477e-bc5d-572cd9f590ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of TfIdf representation for Web of Science Train: (19200, 16997)\n",
      "Shape of TfIdf representation for Web of Science Test: (4800, 16997)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of TfIdf representation for Web of Science Train:', x_train_tfidf.shape)\n",
    "print('Shape of TfIdf representation for Web of Science Test:', x_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Clasificación mediante Naive Bayes [10 puntos]\n",
    "Ahora que tenemos una bolsa de palabras y una representación Tf-Idf de nuestros conjuntos de datos, exploremos cómo se comparan cuando se pasan a un clasificador para predecir la etiqueta del texto. Usaremos dos de las funciones de Naive Bayes de sklearn, Naive Bayes multinomial y NaiveBayes gaussiano.\n",
    "## 3.1: Implementación de Naive Bayes multinomial [5 puntos]\n",
    "En el archivo **classify.py**, complete las siguientes funciones en la clase `MNB`:\n",
    "* <strong>`__init__`</strong>\n",
    "* <strong>fit</strong>\n",
    "* <strong>predict</strong>\n",
    "### 3.1.1 Conjunto de datos de clickbait [Sin puntos]\n",
    "Ejecute la celda a continuación para clasificar la representación de bolsa de palabras del conjunto de datos de clickbait utilizando las funciones de Naive Bayes multinomial que ya ha implementado en 3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 14822,
     "status": "ok",
     "timestamp": 1660193413891,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kVNdpFWfJn0Y",
    "outputId": "ec46e7b4-8203-438a-8dd9-07192c73d144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score for Multinomial Naive Bayes - Clickbait Bag of Words: 0.9774\n",
      "Test Accuracy Score for Multinomial Naive Bayes - Clickbait Bag of Words: 0.9569\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, intentémoslo con la representación tf-idf del conjunto de datos de clickbait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1996,
     "status": "ok",
     "timestamp": 1660193415882,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "uTC7F3RNJn0Z",
    "outputId": "cb6cc280-940e-4838-df5d-a79d9df362ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score for Multinomial Naive Bayes - Clickbait TF-IDF: 0.9784\n",
      "Test Accuracy Score for Multinomial Naive Bayes - Clickbait TF-IDF: 0.9552\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_tfidf, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_tfidf)\n",
    "y_hat_test = clf.predict(x_test_tfidf)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Conjunto de datos de Web of Science [Sin puntos]\n",
    "Ejecute la celda a continuación para clasificar la representación de bolsa de palabras del conjunto de datos de Web of Science utilizando las funciones de Naive Bayes multinomial que ya ha implementado en 3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1660193416658,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "Y5M6VAhwJn0Z",
    "outputId": "cee7d213-387d-425f-8fd1-296868940f68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score for Multinomial Naive Bayes - Web of Science Bag of Words: 0.9750\n",
      "Test Accuracy Score for Multinomial Naive Bayes - Web of Science Bag of Words: 0.8300\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_bow_wos)\n",
    "y_hat_test = clf.predict(x_test_bow_wos)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Web of Science Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, intentémoslo con la representación tf-idf del conjunto de datos de Conjunto de datos de Web of Science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1660193416659,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "d9Du5O62Jn0Z",
    "outputId": "0358bcd1-8360-4525-f1c2-1b06bea63457"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score for Multinomial Naive Bayes - Web of Science TF-IDF: 0.9644\n",
      "Test Accuracy Score for Multinomial Naive Bayes - Web of Science TF-IDF: 0.8500\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_tfidf_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_tfidf_wos)\n",
    "y_hat_test = clf.predict(x_test_tfidf_wos)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Web of Science TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Implementación de Naive Bayes gaussiano [5 puntos]\n",
    "En el archivo **classify.py**, complete las siguientes funciones en la clase `GNB`:\n",
    "* <strong>\\__init__</strong>\n",
    "* <strong>fit</strong>\n",
    "* <strong>predict</strong>\n",
    "### 3.2.1 Conjunto de datos de clickbait [Sin puntos]\n",
    "Ejecute la siguiente celda para clasificar la representación de bolsa de palabras del conjunto de datos de clickbait utilizando las funciones de Naive Bayes gaussiano que ya ha implementado en 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 11773,
     "status": "ok",
     "timestamp": 1660193428427,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "QW_4HZAYJn0a",
    "outputId": "0fef6e39-dc6d-4409-c7fb-d3ddefafe8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score for Gaussian Naive Bayes - Clickbait Bag of Words: 0.9716\n",
      "Test Accuracy Score for Gaussian Naive Bayes - Clickbait Bag of Words: 0.8952\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute la siguiente celda para clasificar la representación de tf-idf del conjunto de datos de clickbait utilizando las funciones de Naive Bayes multinomial que ya ha implementado en 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 10281,
     "status": "ok",
     "timestamp": 1660193438703,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "YMmYX8QNJn0a",
    "outputId": "a694aae4-2bfb-4007-8762-935808b5e800"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'toarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Initialize classifier, fit, and predict\u001b[39;00m\n\u001b[32m      8\u001b[39m clf = GNB()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m clf.fit(\u001b[43mx_train_tfidf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m(), np.array(y_train))\n\u001b[32m     10\u001b[39m y_hat = clf.predict(x_train_tfidf.toarray())\n\u001b[32m     11\u001b[39m y_hat_test = clf.predict(x_test_tfidf.toarray())\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'toarray'"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_tfidf.toarray(), np.array(y_train))\n",
    "y_hat = clf.predict(x_train_tfidf.toarray())\n",
    "y_hat_test = clf.predict(x_test_tfidf.toarray())\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Clickbait TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train, y_hat)\n",
    "clf_test_score = accuracy_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Conjunto de datos de Web of Science [Sin puntos]\n",
    "Ejecute la siguiente celda para clasificar la representación de bolsa de palabras del conjunto de datos de Web of Science utilizando las funciones de Naive Bayes gaussiano que ya ha implementado en 3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1660193440307,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "bQ_Gznv1Jn0b",
    "outputId": "cdf039b1-dd6b-4165-93ee-bf7b13946ea6"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_bow_wos, np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_bow_wos)\n",
    "y_hat_test = clf.predict(x_test_bow_wos)\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Web of Science Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecute la siguiente celda para clasificar la representación tf-idf del conjunto de datos de Web of Science utilizando las funciones Bayesianas Naive Gaussianas que ya ha implementado en 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 1523,
     "status": "ok",
     "timestamp": 1660193441825,
     "user": {
      "displayName": "Rusty Utomo",
      "userId": "03404199279111245878"
     },
     "user_tz": 300
    },
    "id": "kQAeix1sJn0b",
    "outputId": "66d4c770-8489-4f59-f71a-6f5596cfaa58"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import GNB\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = GNB()\n",
    "clf.fit(x_train_tfidf_wos.toarray(), np.array(y_train_wos))\n",
    "y_hat = clf.predict(x_train_tfidf_wos.toarray())\n",
    "y_hat_test = clf.predict(x_test_tfidf_wos.toarray())\n",
    "\n",
    "model = 'Gaussian Naive Bayes'\n",
    "encoding = 'Web of Science TF-IDF'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = accuracy_score(y_train_wos, y_hat)\n",
    "clf_test_score = accuracy_score(y_test_wos, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3: Bayesianas Naive Gaussianas vs. Multinomiales [Sin puntos]\n",
    "Observe la diferencia en la precisión de las pruebas entre Bayesianas Naive Gaussianas (GNB) y Multinomiales (MNB). En general, MNB tiene una precisión de prueba mayor que GNB. Una de las principales suposiciones al utilizar GNB es que las características se distribuyen siguiendo una distribución gaussiana. La distribución gaussiana no se aplica necesariamente a las palabras y puede no ser un algoritmo adecuado para la clasificación de texto. Es importante elegir algoritmos que sean adecuados para el contexto del problema.\n",
    "# Q4: Métricas de evaluación [15 puntos]\n",
    "## 4.1: Implementación de precisión, recuperación, exactitud y puntuación F1 [15 puntos]\n",
    "Para cuantificar la bondad de nuestros métodos de clasificación, necesitamos utilizar métricas de evaluación.\n",
    "\n",
    "En el archivo **metrics.py**, complete las siguientes funciones:\n",
    "* <strong>accuracy</strong>\n",
    "* <strong>recall</strong>\n",
    "* <strong>precision</strong>\n",
    "* <strong>f1_score</strong>\n",
    "* <strong>confusion_matrix</strong>\n",
    "* <strong>roc_auc_score</strong>\n",
    "\n",
    "Puede utilizar sklearn.metrics para todos los métodos de evaluación EXCEPTO para la precisión.\n",
    "\n",
    "La precisión debe implementarse utilizando numpy como la relación entre la cantidad de puntos de datos predichos correctamente y la cantidad total de puntos de datos.\n",
    "\n",
    "$$Precisión = \\dfrac{TP + TN}{TP + TN + FP + FN}$$\n",
    "\n",
    "Donde TP = Verdaderos positivos, TN = Verdaderos negativos, FP = Falsos positivos, FN = Falsos negativos\n",
    "tienes ## 4.2: Analizar el método Naive Bayes multinomial utilizando el conjunto de datos Clickbait [Sin puntos]\n",
    "### 4.2.1 Análisis de la precisión [Sin puntos]\n",
    "Aquí puede ver cómo se compara su implementación de precisión con los valores de precisión de sklearn de la sección 3.1.1. Deben ser idénticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMGwT3_y3RvT",
    "outputId": "289ce862-8db1-49be-b7ba-afb7e75ec971"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "from metrics import Metrics\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "metric = 'Accuracy Score'\n",
    "\n",
    "# Assess performance of classifier using Accuracy score as the metric\n",
    "clf_train_score = Metrics().accuracy(y_train, y_hat)\n",
    "clf_test_score = Metrics().accuracy(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Análisis de recuperación, precisión y puntuación F1 [Sin puntos]\n",
    "Ahora analicemos la recuperación, precisión y puntuación f1 promedio del algoritmo Naive Bayes multinomial utilizando el conjunto de datos Clickbait.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pF2aEl6I3RvT",
    "outputId": "39aeb645-483f-4a48-e9e4-745fa6c08787"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from classify import MNB\n",
    "from metrics import Metrics\n",
    "\n",
    "# Initialize classifier, fit, and predict\n",
    "clf = MNB()\n",
    "clf.fit(x_train_bow, np.array(y_train))\n",
    "y_hat = clf.predict(x_train_bow)\n",
    "y_hat_test = clf.predict(x_test_bow)\n",
    "\n",
    "model = 'Multinomial Naive Bayes'\n",
    "encoding = 'Clickbait Bag of Words'\n",
    "\n",
    "metric = 'Average Recall Score'\n",
    "clf_train_score = Metrics().recall(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().recall(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))\n",
    "print()\n",
    "\n",
    "metric = 'Average Precision Score'\n",
    "clf_train_score = Metrics().precision(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().precision(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))\n",
    "print()\n",
    "\n",
    "\n",
    "metric = 'Average F1 Score'\n",
    "clf_train_score = Metrics().f1_score(y_train, y_hat, average='macro')\n",
    "clf_test_score = Metrics().f1_score(y_test, y_hat_test, average='macro')\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Análisis de la puntuación ROC AUC y la matriz de confusión [Sin puntos]\n",
    "Ahora analicemos la puntuación roc_auc promedio y la matriz de confusión del algoritmo Naive Bayes multinomial utilizando el conjunto de datos Clickbait.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "metric = 'ROC_AUC Score'\n",
    "clf_train_score = Metrics().roc_auc_score(y_train, y_hat)\n",
    "clf_test_score = Metrics().roc_auc_score(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_train_score))\n",
    "print('Test {} for {} - {}: {:.4f}'.format(metric, model, encoding, clf_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd2Bkikp3RvT"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "metric = 'Confusion Matrix'\n",
    "clf_train_confusion_matrix = Metrics().confusion_matrix(y_train, y_hat)\n",
    "clf_test_confusion_matrix = Metrics().confusion_matrix(y_test, y_hat_test)\n",
    "print('Train {} for {} - {}'.format(metric, model, encoding))\n",
    "print(clf_train_confusion_matrix)\n",
    "print('Test {} for {} - {}'.format(metric, model, encoding))\n",
    "print(clf_test_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5: Limpieza de texto para macrodatos [7,5 puntos de crédito adicional]\n",
    "\n",
    "Ahora que hemos explorado la limpieza, clasificación y evaluación de texto, exploremos cómo funcionan en el mundo real cuando hay muchos más datos. Específicamente, exploraremos la biblioteca Hugging Face, una biblioteca popular en aprendizaje automático.\n",
    "## 5.1 Transmisión y tokenización\n",
    "\n",
    "Para este problema, preprocesaremos el conjunto de datos de Wikipedia de Hugging Face. Tomaremos el texto de cada uno de los artículos y usaremos el tokenizador DistilBert para tokenizar el texto. Luego crearemos un conjunto de datos que consista solo en el id, el título y los primeros 100 tokens **en inglés** (¡no numéricos!) del texto del artículo. Un conjunto de datos como este se podría usar para crear un resumidor liviano, pasarlo a un modelo para puntuar la atención del usuario prevista o para analizar los patrones sintácticos que se encuentran en las introducciones de los artículos de Wikipedia.\n",
    "\n",
    "Sin embargo, este conjunto de datos ocupa 20 GB, ¡demasiado grande para caber en la RAM de la mayoría de las computadoras! Por lo tanto, tendremos que **transmitir** el conjunto de datos a nuestras computadoras, como se hace a menudo en el mundo real cuando se crean LLM.\n",
    "\n",
    "En esta sección, completaremos los métodos en el archivo **preprocess_bigdata.py**.\n",
    "### 5.1.1 Inicializar el conjunto de datos [2,5 puntos de bonificación adicional]\n",
    "\n",
    "En el archivo **preprocess_bigdata.py**, cambia la inicialización de **self.dataset** en el método **init** para permitir la transmisión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import test_init_dataset\n",
    "\n",
    "print(\"Starting streaming tests, if the dataset test takes long please double check your code.\")\n",
    "\n",
    "test_pp = Preprocess()\n",
    "test_init_dataset(str(type(test_pp.dataset)))\n",
    "\n",
    "print(\"The columns of your dataset are: \", test_pp.dataset.column_names)\n",
    "print(\"Example row: \")\n",
    "try:\n",
    "    print(next(iter(test_pp.dataset)))\n",
    "except:\n",
    "    print(\"Your dataset is initalized wrong or might be empty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Tokenizar [2,5 puntos de bonificación adicional]\n",
    "\n",
    "En el archivo **preprocess_bigdata.py**:\n",
    "- Inicializa **self.tokenizer** en el método **init**\n",
    "- Completa el método **tokenize**\n",
    "\n",
    "En esta sección, queremos crear un tokenizador que pueda recibir lotes de datos y devolver matrices que contengan los primeros 100 tokens del texto del artículo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import  test_init_tokenizer, tokenizer_example_sentences, test_tokenize\n",
    "\n",
    "test_pp = Preprocess()\n",
    "test_init_tokenizer(str(type(test_pp.tokenizer)))\n",
    "\n",
    "output = test_pp.tokenize(tokenizer_example_sentences, max_length=10)\n",
    "test_tokenize(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Tokenizar el conjunto de datos [2,5 puntos de bonificación adicional]\n",
    "\n",
    "En el archivo **preprocess_bigdata.py**:\n",
    "- Completa el método **preprocess_text**\n",
    "\n",
    "Para este método, utiliza el procesamiento por lotes para devolver el conjunto de datos preprocesado. Para ver por qué necesitamos el procesamiento por lotes, ejecute las dos celdas siguientes y observe cuánto tiempo tardan en ejecutarse:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "dataset_head = test_pp.dataset.take(1)\n",
    "print(list(dataset_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "dataset_head2 = test_pp.dataset.take(10)\n",
    "print(list(dataset_head2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toman aproximadamente el mismo tiempo en ejecutarse, ¡aunque una extrae mucho más a la vez! Por eso el procesamiento por lotes es tan importante, especialmente para las aplicaciones de transmisión: nos permite procesar nuestro conjunto de datos tan rápido como podemos manejarlo, según las limitaciones de memoria de nuestra máquina local.\n",
    "\n",
    "Ahora continúe y pruebe su implementación de **preprocess_text** en la siguiente celda:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from preprocess_bigdata import Preprocess\n",
    "from local_tests.preprocess_bigdata_test import test_init_dataset, test_preprocess\n",
    "\n",
    "test_pp = Preprocess()\n",
    "dataset_cleaned = test_pp.preprocess_text()\n",
    "\n",
    "test_init_dataset(str(type(dataset_cleaned)))\n",
    "\n",
    "first_row = next(iter(dataset_cleaned))\n",
    "test_preprocess(first_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Felicitationes, terminaste la tarea 2 &#128512;!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bFre8YfxMevJ",
    "7P_iMpymNmbt",
    "Fq-y6JE7Jn0Y"
   ],
   "machine_shape": "hm",
   "name": "HW1.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "inf02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
